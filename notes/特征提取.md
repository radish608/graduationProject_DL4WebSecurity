# 特征提取

## 1 数字型特征提取

```python
>>> from sklearn import preprocessing
>>> import numpy as np
>>> x = np.array([[1., -1., 2.],
...               [2., 0., 0.],
...               [0., 1., -1.]])
```



- 标准化

	```python
	>>> x_scaled = preprocessing.scale(x)
	>>> x_scaled
	array([[ 0.        , -1.22474487,  1.33630621],
	       [ 1.22474487,  0.        , -0.26726124],
	       [-1.22474487,  1.22474487, -1.06904497]])
	```

	

- 正则化

	```python
	>>> x_normalized=preprocessing.normalize(x, norm='l2')
	>>> x_normalized
	array([[ 0.40824829, -0.40824829,  0.81649658],
	       [ 1.        ,  0.        ,  0.        ],
	       [ 0.        ,  0.70710678, -0.70710678]])
	```

	

- 归一化

	```python
	>>> x_train_minmax = min_max_scaler.fit_transform(x)
	>>> x_train_minmax
	array([[ 0.5       ,  0.        ,  1.        ],
	       [ 1.        ,  0.5       ,  0.33333333],
	       [ 0.        ,  1.        ,  0.        ]])
	```

	

## 2 文本型特征提取

- 词集模型

	```python
	>>> from sklearn.feature_extraction import DictVectorizer
	>>> measurements = [
	... {'city':'Dubai', 'temperature':33.},
	... {'city':'London', 'temperature':12.},
	... {'city':'San Fransisco', 'temperature':18.},
	... ]
	>>> vec = DictVectorizer()
	>>> vec.fit_transform(measurements).toarray()
	array([[  1.,   0.,   0.,  33.],
	       [  0.,   1.,   0.,  12.],
	       [  0.,   0.,   1.,  18.]])
	>>> vec.get_feature_names()
	['city=Dubai', 'city=London', 'city=San Fransisco', 'temperature']
	>>> vec
	DictVectorizer(dtype=<type 'numpy.float64'>, separator='=', sort=True,
	        sparse=True)
	
	```

	

- 词袋模型

	```python
	>>> from sklearn.feature_extraction.text import CountVectorizer
	>>> vectorizer = CountVectorizer(min_df=1)
	>>> vectorizer
	CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',
	        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',
	        lowercase=True, max_df=1.0, max_features=None, min_df=1,
	        ngram_range=(1, 1), preprocessor=None, stop_words=None,
	        strip_accents=None, token_pattern=u'(?u)\\b\\w\\w+\\b',
	        tokenizer=None, vocabulary=None)
	>>> corpus = [
	... 'This is the first document.',
	... 'This is the second second document.',
	... 'And the third one.',
	... 'Is this the first doucument?',
	... ]
	>>> X = vectorizer.fit_transform(corpus)
	>>> X
	<4x10 sparse matrix of type '<type 'numpy.int64'>'
		with 19 stored elements in Compressed Sparse Row format>
	>>> vectorizer.get_feature_names()
	[u'and', u'document', u'doucument', u'first', u'is', u'one', u'second', u'the', u'third', u'this']
	>>> X.toarray()
	array([[0, 1, 0, 1, 1, 0, 0, 1, 0, 1],
	       [0, 1, 0, 0, 1, 0, 2, 1, 0, 1],
	       [1, 0, 0, 0, 0, 1, 0, 1, 1, 0],
	       [0, 0, 1, 1, 1, 0, 0, 1, 0, 1]])
	```

	词袋的特征空间叫`vocabulary`：`vocabulary=vectorizer.vocabulary_`针对其他文本进行词袋处理时，可以直接使用现有词汇表

## 3 数据读取

TensorFlow提供从CSV中读取数据集

```python
import tensorflow as tf
import numpy as np
traning_set = tf.contrib.learn.datasets.base.load_csv_with_header(
              filename="*.csv", #文件名
              target_dtype=np.int, #标记数据类型
              features_dtype=np.float32) #特征数据类型
feature_colunms = [tf.contrib,layers.real_valued_column("", dimension=4)]
```

访问数据集合的特征以及标记的方式为：

```python
x = training_set.data
y = training_set.targer
```


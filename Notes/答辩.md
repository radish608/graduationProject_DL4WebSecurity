# 基于深度学习的Web安全应用

精准度（precision）：
P = TP/(TP+FP) 指被分类器判定正例中的正样本的比重

召回率(Recall)：
R=TP/(TP+FN) = 1- FN/T 指的是被预测为正例的占总的正例的比重

准确率(Accuracy)：

A = TP/(TP+FN)=(TP+TN)/(TP+FN+FP+TN) 反映了分类器对整个样本的判定能力，也就是说能将正的判定为正，负的判定为负

计算出准确率和召回率后，就能得到F1值，它是两者的调和平均数。  

## 1 研究背景（选题原因）

### 1.1Web安全

- Web安全在社会中的地位日益凸显。

  社会的正常运转已经离不开Web，比如电子政务、部分企业服务很多都基于Web架构，

- Web安全形势越来越严峻

  - 漏洞增多

    Web应用越来越多，因此也带来了更多攻击入口

  - 黑产成熟

    网络黑产产业链越来越完善，于是犯罪门槛降低，隐蔽性更强，效率提高

  - 从业人员缺口增大

    安全业务量和业务面相应增加和拓宽，对安全从业人员的需求越来越大，对安全从业人员的能力、素质、经验等方面的要求越来越高。而当下安全从业人员中，脚本小子数量过度饱和，真正意义上的优秀人才数量匮乏，在国内大多聚集在部分大型互联网公司，相关专业毕业生的能力水平并不能直接满足岗位需求。  

  - AI+攻击出现

    曾有不法分子将人工智能技术应用在诱骗来的注册信息中，并形成了完整的全链条犯罪，包撞库、盗号、贩卖公民信息、网络诈骗等。利用人工智能技术进行攻击，可以快速发动规模宏大的高威胁攻击。仅仅靠人工应对这种攻击方式可能实在是螳臂当车，难以应付。 

### 1.2人工智能

通常认为，机器学习是实现人工智能的主要方式，人类基于机器学习以及海量的数据，逐步实现人工智能，其中深度学习是机器学习的一个分支。与传统的机器学习（Machine Learning，ML）技术相比，深度学习（Deep Learning，DL）技术的泛化能力更好。 

*泛化能力*（generalization ability）是指机器学习算法对新鲜样本的适应*能力*。学习的目的是学到隐含在数据对背后的规律，对具有同一规律的学习集以外的数据，经过训练的网络也能给出合适的输出，该*能力*称为*泛化能力*。 

## 2 研究内容

主要研究了验证码识别、垃圾邮件识别、Linux后门检测、WebShell检测这四个案例，并用深度学习技术解决这四个实际案例。 

## 3 验证码识别

### 3.1数据集

使用python的captchalib库生成。

首先，确定验证码图片中的字符数量、集合、图像尺寸。本数据集中使用的字符数为4个，从0-9、a-z、A-Z中选择，图片大小为100*60。接着，使用random库随机选择4个字符，生成字符串。最后，保存生成的图片。同时，为了便于提取标签，将随机字符串写在文件名中。文件名格式为“验证码字符_生成时间.png”。 

### 3.2特征提取

由于图片是彩色的，其文件本质是RGB三维数组，因此要将其灰度化。在判断图像为RGB图像（即判断数组大于二维）之后，分别读取array[:, :, 0]、array [:, :, 1]、array [:, :, 2]到变量r、g、b，并计算0.2989*r+0.5870*g+0.1140*b，所得到的二维数组即为灰度化结果。 最后将数组转化成OneHot编码。OneHot编码可以让离散数据映射到欧几里得空间，从而易于被分类器处理，同时也扩充了特征。 

### 3.3训练与效果验证

基于卷积神经网络 。

在本算法中，共有3个卷积层，2个全连接层，实现基于TensorFlow。处理过程如下：

- 定义输入为[-1, 100, 60, 1]从而获得100x60的张量。

- 使用卷积处理，卷积核大小为3x3，卷积核数量为32，可以理解为通过3x3大小的卷积操作，提取32种特征，处理后的张量大小为100x60x32，激励函数使用relu。

  在卷积神经网络中通常采用Relu作为激励函数。这是由于Relu函数结构简单， 训练过程中可以大幅提高训练速度。在小于零部分输出值为0，在训练过程中使一些神经元不起作用，降低了过拟合的概率。

- 使用池化处理，池化大小为2x2，处理后的张量大小为50x60x32。为了避免过拟合，设置Dropout为0.75。池化即从每一维中提取最大值表示该维的特征。

  引入池化层不仅大幅减少训练参数，降低训练复杂度，提高训练速度，而且模型的性能更高效。 

- 使用卷积处理，卷积核大小为3x3，卷积核数量为64，可以理解为通过3x3大小的卷积操作，提取64种特征，处理后的张量大小为50x60x64，激励函数使用relu。

- 使用池化处理，池化大小为2x2，处理后的大小为25x30x64。再次设置Dropout为0.75。

- 使用卷积处理，卷积核大小为3x3，卷积核数量为128，可以理解为通过3x3大小的卷积操作，提取128种特征，处理后的张量大小为25x30x128，激励函数使用relu。

- 使用池化处理，池化大小为2x2，处理后的大小为13x15x128。再次设置Dropout为0.75。

- 将张量标准化，即将大小为12x15x128的张量转化为长度为23040的一维张量，并与节点数为1024的隐藏层进行全连接，激励函数为relu，设置Dropout为0.75。

- 再与节点数为1024的隐藏层进行全连接，获得长度为1024的一维张量。在这一步，CNN完成了大小为100x60的张量到大小为1x1024的张量的训练转化。

- 使用adam优化器进行优化，学习率为0.0001，损失函数为sigmoid_cross_entropy_with_logits()。

  深度学习需要耗费大量的时间与计算机资源，为了加速机器的学习速度和效果，需要令模型收敛更快的最优化算法。用于评估参数好坏的损失函数是需要更新权重的复合函数，并且该复合函数是凸函数，经过大量实验论证Adam优化算法应用在非凸优化问题中具有明显优势。传统的优化算法在随机梯度下降中使用固定的学习率来更新所有的权重，而Adam优化算法能在训练过程中不断调整学习率使训练过程更优化。 

  sigmoid_cross_entropy_with_logits()，该函数不仅可以用于二分类，也可以用于多分类，例如：判断图片中是否包含几种动物中的一种或多种。

在经过25轮共990步训练后，验证集准确率95.99%。

## 4 垃圾邮件检测 

### 4.1数据集

公开搜索

### 4.2特征提取

### 4.3训练

词向量的表示自然语言的方式，如[ 0.792, -0.177, -0.107, 0.109, -0.543, … ]。这种表示方式让相关或者相似的词在距离上更接近。词向量的方式是目前基于深度学习的自然语言处理普遍使用的方法，这种表示方法先对于独热码的优势在于：第一，使用独热码编码方式，任意两个单词之间都是孤立的，词向量表示法弥补了这个缺陷，相关联的词在逻辑距离上相近，这更符合自然语言的特点；第二，独热码采用稀疏编码的方式表示词，耗费资源大，不利于网络的训练，使用低维的词向量就可以避免此类问题。单词向量通常是通过训语言建模的方式获取的。 

全连接层用softmax激活函数。分类中使用多。

to_categorical(2)，处理更方便，端到端的效果



## MLP:注意：默认解算器“adam”在相对较大的数据集（包含数千个训练样本或更多）方面在训练时间和验证分数方面都能很好地工作。但是，对于小型数据集，“lbfgs”可以更快地收敛并且表现更好。 

